{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8358ab18-9189-4720-ad0e-b188f375ab42",
   "metadata": {},
   "source": [
    "# 361 Assignment 1 - James Pirie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddcc2f7-5661-4eb1-aaa6-f4013ba7644c",
   "metadata": {},
   "source": [
    "### Task 1 Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e400778-9e6e-4765-8752-686d1f76fded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from baseline_model import decision_tree_implementation\n",
    "\n",
    "\n",
    "def read_data(filename: str, features: str):\n",
    "    \"\"\"Creating a DataFrame from a dictionary\"\"\"\n",
    "    df = pd.read_csv(filename, header=None, names=features)\n",
    "    df = df[~df.isin(['?']).any(axis=1)]\n",
    "    df = df.sample(frac=1, random_state=4).reset_index(drop=True)  # Shuffle the DataFrame with the provided seed\n",
    "    return df\n",
    "\n",
    "\n",
    "def entropy(data_input: pd.DataFrame):\n",
    "    \"\"\"Calculate the entropy of a DataFrame\"\"\"\n",
    "    total_rows = data_input.shape[0]\n",
    "    is_poisonous = data_input[data_input.iloc[:, 0] == 'p'].shape[0]\n",
    "    is_not_poisonous = data_input[data_input.iloc[:, 0] == 'e'].shape[0]\n",
    "\n",
    "    if is_poisonous == 0 or is_not_poisonous == 0:\n",
    "        return 0\n",
    "\n",
    "    if is_poisonous == is_not_poisonous:\n",
    "        return 1\n",
    "\n",
    "    # equation for entropy\n",
    "    return - (is_poisonous / total_rows) * math.log2(is_poisonous / total_rows)\\\n",
    "           - (is_not_poisonous / total_rows) * math.log2(is_not_poisonous / total_rows)\n",
    "\n",
    "\n",
    "def information_gain(data_input: pd.DataFrame):\n",
    "    \"\"\"Calculate the information gain for every value of a feature in a DataFrame\"\"\"\n",
    "    # entropy of entire column\n",
    "    entropy_before_split = entropy(data_input)\n",
    "    total_size = data_input.shape[0]  # number of total rows\n",
    "\n",
    "    all_possible_values = data_input.iloc[:, 1].unique().tolist()\n",
    "    weighted_entropy_sum = 0\n",
    "\n",
    "    for value in all_possible_values:\n",
    "        # split the data based on the value to split the column\n",
    "        data_split_on_value = data_input[data_input.iloc[:, 1] == value]\n",
    "        data_split_on_value_size = data_split_on_value.shape[0]  # get size of new DataFrame\n",
    "\n",
    "        weighted_entropy_sum += (data_split_on_value_size / total_size) * entropy(data_split_on_value)\n",
    "\n",
    "    # calculate and return information gain\n",
    "    return entropy_before_split - weighted_entropy_sum\n",
    "\n",
    "\n",
    "def find_split(data_input: pd.DataFrame):\n",
    "    \"\"\"Find the largest information-gain in a DataFrame\"\"\"\n",
    "    poisonous = data_input.iloc[:, 0]  # get the values from the first column (poisonous column)\n",
    "    features = data_input.iloc[:, 1:]  # remove classifier value from the list\n",
    "    information_gains = []\n",
    "\n",
    "    # iterate through every column\n",
    "    for feature in features:\n",
    "        current_table = data_input.loc[:, [poisonous.name, feature]]\n",
    "        current_information_gain = {\"feature\": feature,\n",
    "                                    \"information_gain\": information_gain(current_table)}\n",
    "        information_gains.append(current_information_gain)\n",
    "\n",
    "    # return the largest information gain\n",
    "    return max(information_gains, key=lambda x: x['information_gain'])\n",
    "\n",
    "\n",
    "def split_data(data_input: pd.DataFrame, largest_information_gain: dict):\n",
    "    \"\"\"Split a DataFrame based upon an information gain score\"\"\"\n",
    "    all_possible_values = data_input[largest_information_gain['feature']].unique().tolist()\n",
    "    split_frames = []\n",
    "    for value in all_possible_values:\n",
    "        split_with_value = data_input[data_input[largest_information_gain['feature']] == value]\n",
    "        split_frames.append(split_with_value)\n",
    "\n",
    "    return split_frames\n",
    "\n",
    "\n",
    "def get_majority(data_input: pd.DataFrame):\n",
    "    return data_input.iloc[:, 0].value_counts().idxmax()\n",
    "\n",
    "\n",
    "def greedy_recursive_splitting(data_input: pd.DataFrame, stopping_depth: int, current_depth: int) -> str:\n",
    "    \"\"\"Recursively split the DataFrame until either entropy is zero or depth limit has been reached \"\"\"\n",
    "    tab = \"    \"\n",
    "    current_depth += 1\n",
    "    splitting_point = find_split(data_input)\n",
    "\n",
    "    all_values_to_split = data_input[splitting_point['feature']].unique().tolist()\n",
    "    frame_index = 0\n",
    "\n",
    "    output = \"\"\n",
    "\n",
    "    if entropy(data_input) != 0:\n",
    "        for value in all_values_to_split:\n",
    "            output += f\"{current_depth * tab}if mushroom['{splitting_point['feature']}'] == '{value}':\\n\"\n",
    "            split_frames = split_data(data_input, splitting_point)\n",
    "\n",
    "            if current_depth == stopping_depth or entropy(split_frames[frame_index]) == 0:\n",
    "                output += f'{(current_depth + 1) * tab }return \"{get_majority(split_frames[frame_index])}\"\\n'\n",
    "            else:\n",
    "                for frame in split_frames:\n",
    "                    if current_depth <= stopping_depth:\n",
    "                        output += greedy_recursive_splitting(frame, stopping_depth, current_depth)\n",
    "            frame_index += 1\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def train(data_input: pd.DataFrame, stopping_depth: int):\n",
    "    \"\"\"Take in a DataFrame and return a string of the code for the nodes for the decision tree\"\"\"\n",
    "    tree_setup = \\\n",
    "        \"import pandas as pd\\n\\n\\ndef decision_tree_implementation(mushroom: pd.DataFrame):\\n\"\n",
    "    return tree_setup + greedy_recursive_splitting(data_input, stopping_depth, 0)\n",
    "\n",
    "\n",
    "def evaluate_model(test_data: pd.DataFrame, decision_tree_implementation):\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    true_negative = 0\n",
    "    false_negative = 0\n",
    "    correct = 0\n",
    "\n",
    "    for i in range(test_data.shape[0]):\n",
    "        if test_data.iloc[i]['poisonous'] == decision_tree_implementation(test_data.iloc[i]):\n",
    "            correct += 1\n",
    "\n",
    "        if test_data.iloc[i]['poisonous'] == 'p' and decision_tree_implementation(test_data.iloc[i]) == 'p':\n",
    "            true_positive += 1\n",
    "\n",
    "        elif test_data.iloc[i]['poisonous'] == 'p' and decision_tree_implementation(test_data.iloc[i]) == 'e':\n",
    "            false_negative += 1\n",
    "\n",
    "        elif test_data.iloc[i]['poisonous'] == 'e' and decision_tree_implementation(test_data.iloc[i]) == 'e':\n",
    "            true_negative += 1\n",
    "\n",
    "        elif test_data.iloc[i]['poisonous'] == 'e' and decision_tree_implementation(test_data.iloc[i]) == 'p':\n",
    "            false_positive += 1\n",
    "\n",
    "    accuracy = (correct / test_data.shape[0]) * 100\n",
    "\n",
    "    if true_positive + false_positive != 0:\n",
    "        precision = (true_positive / (true_positive + false_positive))\n",
    "        precision = round(precision, 2)\n",
    "    else:\n",
    "        precision = 'undefined'\n",
    "\n",
    "    if true_positive + false_negative != 0:\n",
    "        recall = (true_positive / (true_positive + false_negative))\n",
    "        recall = round(recall, 2)\n",
    "    else:\n",
    "        recall = 'undefined'\n",
    "\n",
    "    try:\n",
    "        f_score = 2 * ((precision * recall) / (precision + recall))\n",
    "        f_score = round(f_score, 2)\n",
    "    except:\n",
    "        f_score = 'undefined'\n",
    "\n",
    "    accuracy = round(accuracy, 2)\n",
    "    return f\"Test Accuracy: {accuracy}%, Precision: {precision}, Recall: {recall}, F-Score: {f_score}\"\n",
    "\n",
    "\n",
    "def test(test_data: pd.DataFrame):\n",
    "    # import here, in case tree has just been trained\n",
    "    from tree_implementation import decision_tree_implementation\n",
    "    return evaluate_model(test_data, decision_tree_implementation)\n",
    "\n",
    "\n",
    "def baseline_test(test_data: pd.DataFrame):\n",
    "    return evaluate_model(test_data, decision_tree_implementation)\n",
    "\n",
    "\n",
    "def k_fold_cross_validation(k: int, data_input: pd.DataFrame, depth):\n",
    "    \"\"\"Divide the dataset into k partitions, \"\"\"\n",
    "    k_data_frames = np.array_split(data_input, k)\n",
    "\n",
    "    for i in range(len(k_data_frames)):\n",
    "        # use 1/kth of the data for testing\n",
    "        testing_frame = pd.DataFrame(k_data_frames[i])\n",
    "        # use the rest of the data for training\n",
    "        training_frame = pd.concat([frame for j, frame in enumerate(k_data_frames) if j != i])\n",
    "        print(f\"======================================= Test {i + 1} =======================================\")\n",
    "        write_to_file(train(training_frame, depth))\n",
    "        print(f\"Model Test: {test(testing_frame)}\")\n",
    "        print(f\"Baseline Test: {baseline_test(testing_frame)}\")\n",
    "\n",
    "\n",
    "def write_to_file(decision_tree: str):\n",
    "    \"\"\"Write the decision tree to a .py file\"\"\"\n",
    "    with open('tree_implementation.py', 'w') as file:\n",
    "        file.write(decision_tree)\n",
    "\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa810b09-439f-4a1c-8533-df0c48f868a4",
   "metadata": {},
   "source": [
    "## Task 1 Part A & B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28ccd9d2-e86c-47cd-bf79-829e1fcbb944",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['poisonous', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n",
    "                'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root',\n",
    "                'stalk-surface-above-ring', 'stalk-surface-below-ring', 'stalk-color-above-ring',\n",
    "                'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type',\n",
    "                'spore-print-color',\n",
    "                'population', 'habitat']\n",
    "\n",
    "data = read_data('mushroom/agaricus-lepiota.data', column_names)\n",
    "split_index = int(len(data) * 0.8)\n",
    "training_data = data.iloc[:split_index]  # 80% of data for training\n",
    "testing_data = data.iloc[split_index:]  # 20% of data for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a0c9ee-e7bc-487a-bc13-3801c9f50c93",
   "metadata": {},
   "source": [
    "### Depth 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "944f6b7b-71e6-4d8f-a61c-3650fcf5c254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "\n",
      "\n",
      "def decision_tree_implementation(mushroom: pd.DataFrame):\n",
      "    if mushroom['odor'] == 'f':\n",
      "        return \"p\"\n",
      "    if mushroom['odor'] == 'n':\n",
      "        if mushroom['spore-print-color'] == 'n':\n",
      "            return \"e\"\n",
      "        if mushroom['spore-print-color'] == 'k':\n",
      "            return \"e\"\n",
      "        if mushroom['spore-print-color'] == 'w':\n",
      "            return \"e\"\n",
      "        if mushroom['spore-print-color'] == 'r':\n",
      "            return \"p\"\n",
      "    if mushroom['odor'] == 'a':\n",
      "        return \"e\"\n",
      "    if mushroom['odor'] == 'l':\n",
      "        return \"e\"\n",
      "    if mushroom['odor'] == 'c':\n",
      "        return \"p\"\n",
      "    if mushroom['odor'] == 'p':\n",
      "        return \"p\"\n",
      "    if mushroom['odor'] == 'm':\n",
      "        return \"p\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train(training_data, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc357cf-53f8-471d-9ea1-4a64dc052532",
   "metadata": {},
   "source": [
    "### Depth 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef1b8373-2262-43cb-8c6f-4888567f126f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "\n",
      "\n",
      "def decision_tree_implementation(mushroom: pd.DataFrame):\n",
      "    if mushroom['odor'] == 'f':\n",
      "        return \"p\"\n",
      "    if mushroom['odor'] == 'n':\n",
      "        if mushroom['spore-print-color'] == 'n':\n",
      "            return \"e\"\n",
      "        if mushroom['spore-print-color'] == 'k':\n",
      "            return \"e\"\n",
      "        if mushroom['spore-print-color'] == 'w':\n",
      "            if mushroom['cap-color'] == 'n':\n",
      "                return \"e\"\n",
      "            if mushroom['cap-color'] == 'c':\n",
      "                return \"e\"\n",
      "            if mushroom['cap-color'] == 'y':\n",
      "                return \"p\"\n",
      "            if mushroom['cap-color'] == 'w':\n",
      "                return \"p\"\n",
      "            if mushroom['cap-color'] == 'p':\n",
      "                return \"e\"\n",
      "            if mushroom['cap-color'] == 'g':\n",
      "                return \"e\"\n",
      "        if mushroom['spore-print-color'] == 'r':\n",
      "            return \"p\"\n",
      "    if mushroom['odor'] == 'a':\n",
      "        return \"e\"\n",
      "    if mushroom['odor'] == 'l':\n",
      "        return \"e\"\n",
      "    if mushroom['odor'] == 'c':\n",
      "        return \"p\"\n",
      "    if mushroom['odor'] == 'p':\n",
      "        return \"p\"\n",
      "    if mushroom['odor'] == 'm':\n",
      "        return \"p\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train(training_data, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1090a6e5-c4d3-4490-b3c1-09a3afac0a09",
   "metadata": {},
   "source": [
    "### Depth 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f0cc0f0-2d5b-4c1d-9ba6-7072c59490d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "\n",
      "\n",
      "def decision_tree_implementation(mushroom: pd.DataFrame):\n",
      "    if mushroom['odor'] == 'f':\n",
      "        return \"p\"\n",
      "    if mushroom['odor'] == 'n':\n",
      "        if mushroom['spore-print-color'] == 'n':\n",
      "            return \"e\"\n",
      "        if mushroom['spore-print-color'] == 'k':\n",
      "            return \"e\"\n",
      "        if mushroom['spore-print-color'] == 'w':\n",
      "            if mushroom['cap-color'] == 'n':\n",
      "                return \"e\"\n",
      "            if mushroom['cap-color'] == 'c':\n",
      "                return \"e\"\n",
      "            if mushroom['cap-color'] == 'y':\n",
      "                return \"p\"\n",
      "            if mushroom['cap-color'] == 'w':\n",
      "                return \"p\"\n",
      "            if mushroom['cap-color'] == 'p':\n",
      "                return \"e\"\n",
      "            if mushroom['cap-color'] == 'g':\n",
      "                return \"e\"\n",
      "        if mushroom['spore-print-color'] == 'r':\n",
      "            return \"p\"\n",
      "    if mushroom['odor'] == 'a':\n",
      "        return \"e\"\n",
      "    if mushroom['odor'] == 'l':\n",
      "        return \"e\"\n",
      "    if mushroom['odor'] == 'c':\n",
      "        return \"p\"\n",
      "    if mushroom['odor'] == 'p':\n",
      "        return \"p\"\n",
      "    if mushroom['odor'] == 'm':\n",
      "        return \"p\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train(training_data, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f1fd75-7786-4d68-ae76-a7d1b1867042",
   "metadata": {},
   "source": [
    "### Task 1 A Paragraph\n",
    "The train method takes in a dataframe, and outputs a string of python code representing the decision tree. The algorithm implemented is as described in lectures, calculates which feature has the largest information gain, then splits the dataset at said feature for every potential value in the column. This repreats recursivley until the entropy of the dataframe after a split is zero, or as will be used in part B, until the stopping_depth is reached. When stopping the majority class of the data frame is returned, either p or e. This is shown, as will be explaiend in task B Paragrah, in the code snipet labeld Depth 4.\n",
    "\n",
    "### Task 1 B Paragraph\n",
    "For task B, I have implemented the stopping_depth which is taken in as an argument to the train method. The depth is tracked every time the recursion depth increases, and stops when depth reaches stopping depth. Above I display 3 cases, one where depth is 2, second where it is 3 and finaly 4. When depth is set to two the tree only splits on odor and then spore-print-color, before being forced to stop, as we are now at depth 2. For depth 3 it splits one more time at cap-color, before again being forced to stop. Finaly, at depth 4, the tree is the same at depth 3, as this time maximum entropy has been reached in all of the leaf dataframes, and no more splitting is neccesary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e028b42c-cc19-4754-96cf-9a7b947e7c68",
   "metadata": {},
   "source": [
    "## Task 1 Part C & D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b502bb8c-95f1-40ce-b6f6-b3d2df096833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================= Test 1 =======================================\n",
      "Model Test: Test Accuracy: 98.05%, Precision: 1.0, Recall: 0.95, F-Score: 0.97\n",
      "Baseline Test: Test Accuracy: 64.42%, Precision: undefined, Recall: 0.0, F-Score: undefined\n",
      "======================================= Test 2 =======================================\n",
      "Model Test: Test Accuracy: 98.41%, Precision: 1.0, Recall: 0.96, F-Score: 0.98\n",
      "Baseline Test: Test Accuracy: 62.48%, Precision: undefined, Recall: 0.0, F-Score: undefined\n",
      "======================================= Test 3 =======================================\n",
      "Model Test: Test Accuracy: 98.94%, Precision: 1.0, Recall: 0.97, F-Score: 0.98\n",
      "Baseline Test: Test Accuracy: 61.42%, Precision: undefined, Recall: 0.0, F-Score: undefined\n",
      "======================================= Test 4 =======================================\n",
      "Model Test: Test Accuracy: 97.7%, Precision: 1.0, Recall: 0.94, F-Score: 0.97\n",
      "Baseline Test: Test Accuracy: 59.47%, Precision: undefined, Recall: 0.0, F-Score: undefined\n",
      "======================================= Test 5 =======================================\n",
      "Model Test: Test Accuracy: 98.05%, Precision: 1.0, Recall: 0.95, F-Score: 0.97\n",
      "Baseline Test: Test Accuracy: 62.94%, Precision: undefined, Recall: 0.0, F-Score: undefined\n",
      "======================================= Test 6 =======================================\n",
      "Model Test: Test Accuracy: 98.58%, Precision: 1.0, Recall: 0.96, F-Score: 0.98\n",
      "Baseline Test: Test Accuracy: 62.41%, Precision: undefined, Recall: 0.0, F-Score: undefined\n",
      "======================================= Test 7 =======================================\n",
      "Model Test: Test Accuracy: 98.05%, Precision: 1.0, Recall: 0.95, F-Score: 0.97\n",
      "Baseline Test: Test Accuracy: 60.99%, Precision: undefined, Recall: 0.0, F-Score: undefined\n",
      "======================================= Test 8 =======================================\n",
      "Model Test: Test Accuracy: 99.11%, Precision: 1.0, Recall: 0.98, F-Score: 0.99\n",
      "Baseline Test: Test Accuracy: 60.64%, Precision: undefined, Recall: 0.0, F-Score: undefined\n",
      "======================================= Test 9 =======================================\n",
      "Model Test: Test Accuracy: 98.76%, Precision: 1.0, Recall: 0.97, F-Score: 0.98\n",
      "Baseline Test: Test Accuracy: 60.11%, Precision: undefined, Recall: 0.0, F-Score: undefined\n",
      "======================================= Test 10 =======================================\n",
      "Model Test: Test Accuracy: 98.76%, Precision: 1.0, Recall: 0.97, F-Score: 0.98\n",
      "Baseline Test: Test Accuracy: 63.12%, Precision: undefined, Recall: 0.0, F-Score: undefined\n"
     ]
    }
   ],
   "source": [
    "k_fold_cross_validation(k=10, data_input=data, depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a174cee3-d0e9-4f3f-b110-0d62080c47f4",
   "metadata": {},
   "source": [
    "### Task 1 Part C Paragraph\n",
    "To test the tree, I implemented a k fold cross validation function. The function takes in the data, and splits it into k equal sized sets. It then loops throguh every set, and each time the current nth partition is used as the testing data while the rest are used for training. This produces k trees trained on slightly different data. I then calculate the accuracy, precision and recall. The above results is tested at k=10, producing 10 trees trained and tested with different variations of the dataset, the stopping_depth is set to 2. As we can see precision is perfect, accuracy is around 98 percent for every tree and recall between 0.95 and 0.99.\n",
    "### Task 1 Part D Paragraph\n",
    "To evaluate the output, inside the k fold cross validation I calculated the f score and then compared the test results to the baseline model. The baseline model simply predicts the majority class of the entire dataset. As we can see the trained tree is significantly more accurate than the baseline, at around 98% opposed to around 60% respectivley. I also calculated the F score, which can be used to evaluate a model. Consistently the f-score is near 0.98, which is very close to 1. This indicated the model is very succesful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b39341-0dfb-43e4-9b2e-cfd2eeb3733c",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "### Task 2 Paragraph A\n",
    "One interesting modification I could do immediatly to the splitting criteria would be to split on the features with the lowest information gain. Although interesting, this would not be a suitable method to practically train a decision tree. By splitting on the lowest information gain we are barely changing the entropy of our dataset, meaning entropy will always be the highest it could be in a split. High entropy means that any given partition on said tree would be unlikley to separate edible from poisonous. And therefore will make poor predicitons. However, as long as the information gain is not 0 for every split, eventualy, after a significant amount of splits. The tree may develop some degree of accuracy. \n",
    "\n",
    "### Task 3 Paragraph A\n",
    "My model activley works to prevent overfitting. Firstly, it evaluates and tests using 10 fold cross validation, which trains 10 different models, and keeps testing and training data separate. By segregating the data the tree has less opportunity to adapt to the noise of the training set. It should adapt to the trends of the data, not the points. And by doing this 10 times, it ensures that it is simply not overfitting just by chance, where it might once accidentaly make a split that happens to work well with the data set. Instead we can gauge the fit of the model by averging it out across the 10 cross validations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fe72a2-6c55-46fa-8dc6-caa372952da1",
   "metadata": {},
   "source": [
    "### Additional Information\n",
    "If you wish to replicate my program outside of jupiter notebook, you will need to create a python file in the same directory called baseline_model.py and use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a019dba-cc11-475f-9a96-193e95931b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def decision_tree_implementation(mushroom: pd.DataFrame):\n",
    "    \"\"\"Tree implementation\"\"\"\n",
    "    return \"e\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbdc887-723a-4325-8852-97dca4142419",
   "metadata": {},
   "source": [
    "To train, test and validate a model, you will need to run the following, which will create a new file called tree_implememntation.py, and write the newly trained trees to that, before validating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84dea40-7e55-442e-8b68-055898540743",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    column_names = ['poisonous', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n",
    "                    'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root',\n",
    "                    'stalk-surface-above-ring', 'stalk-surface-below-ring', 'stalk-color-above-ring',\n",
    "                    'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type',\n",
    "                    'spore-print-color',\n",
    "                    'population', 'habitat']\n",
    "\n",
    "    data = read_data('mushroom/agaricus-lepiota.data', column_names)\n",
    "    split_index = int(len(data) * 0.8)\n",
    "\n",
    "    k_fold_cross_validation(10, data, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ac513d-4c7f-48f9-8dd8-90d3991ecaf1",
   "metadata": {},
   "source": [
    "### Requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61a9540-3e75-4511-989e-982b780b9ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy\n",
    "pandas\n",
    "ucimlrepo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
